
1
00:00:00.000 --> 00:00:03.600
I've noticed that almost all the really good machine learning
我注意到 几乎所有真正优秀的机器学习者

2
00:00:03.600 --> 00:00:07.890
practitioners tend to be very sophisticated in understanding of Bias and Variance.
对偏差(偏离度)和方差(集中度)的处理都是非常有经验的

3
00:00:07.890 --> 00:00:12.330
Bias and Variance is one of those concepts that's easily learned but difficult to master.
偏差和方差的处理往往非常容易入门 但是非常难以精通

4
00:00:12.330 --> 00:00:16.155
Even if you think you've seen the basic concepts of Bias and Variance,
即使你认为你已经看完了偏差和方差的基础理论

5
00:00:16.155 --> 00:00:18.805
there's often more new ones to it than you'd expect.
也常常会发现总会有一些出乎你意料的东西

6
00:00:18.805 --> 00:00:20.620
In the Deep Learning Area,
在深度学习领域

7
00:00:20.620 --> 00:00:22.650
another trend is that there's been
另一个现象是有关

8
00:00:22.650 --> 00:00:26.035
less discussion of what's called the bias-variance tradeoff.
偏差-方差权衡(也有叫做偏差-方差困境)的讨论很少

9
00:00:26.035 --> 00:00:28.657
You might have heard this thing called the bias-variance tradeoff.
你可能之前已经听说过这个被称为“偏差-方差权衡”的东西

10
00:00:28.657 --> 00:00:31.330
But in Deep Learning Area there's less of a tradeoff,
但是在深度学习领域 这不仅仅是两者间的权衡问题

11
00:00:31.330 --> 00:00:32.520
so we'd still talk of the bias,
我们仍然讨论偏差

12
00:00:32.520 --> 00:00:33.865
we still talk of the variance,
也仍然讨论方差

13
00:00:33.865 --> 00:00:37.295
but we just talk less about the bias-variance tradeoff.
但是我们对偏差-方差权衡要讲的不多

14
00:00:37.295 --> 00:00:39.795
Let's see what this means.
让我们一起来看看为什么这么说

15
00:00:39.795 --> 00:00:42.750
Let's see the data set that looks like this.
大家应该看到了屏幕上这些训练集

16
00:00:42.750 --> 00:00:44.800
If you fit a straight line to the data,
如果你用一条直线来区分样本数据

17
00:00:44.800 --> 00:00:48.130
maybe get a logistic regression fit to that.
那么用逻辑回归可能画出图上的这条直线

18
00:00:48.130 --> 00:00:50.415
This is not a very good fit to the data.
这和训练数据的拟合度并不高

19
00:00:50.415 --> 00:00:52.380
And so this is class of a high bias,
这样的分类我们称之为高偏差

20
00:00:52.380 --> 00:00:56.400
what we say that this is underfitting the data.
或者换一种说法 这是欠拟合

21
00:00:56.400 --> 00:00:57.850
On the opposite end,
相对的

22
00:00:57.850 --> 00:01:00.645
if you fit an incredibly complex classifier,
如果你使用一个极为复杂的分类器

23
00:01:00.645 --> 00:01:02.640
maybe deep neural network,
例如深度神经网络

24
00:01:02.640 --> 00:01:05.995
or neural network with all the hidden units,
或者含有所有隐藏神经元的神经网络

25
00:01:05.995 --> 00:01:10.255
maybe you can fit the data perfectly,
或许你可以像图上画的这样完美区分训练数据

26
00:01:10.255 --> 00:01:12.220
but that doesn't look like a great fit either.
但那看上去也并不是一个非常好的分类算法

27
00:01:12.220 --> 00:01:17.535
So there's a classifier of high variance and this is overfitting the data.
这个高方差的分类器 我们也称之为过拟合

28
00:01:17.535 --> 00:01:19.650
And there might be some classifier in between,
在这两种分类器之间

29
00:01:19.650 --> 00:01:22.070
with a medium level of complexity,
我们应该能找到一种不那么复杂的

30
00:01:22.070 --> 00:01:24.585
that maybe fits it correctly like that.
但是能正确分类的算法 像图上这样

31
00:01:24.585 --> 00:01:27.300
That looks like a much more reasonable fit to the data,
这看起来对数据的区分非常合理

32
00:01:27.300 --> 00:01:31.705
so we call that just right. It's somewhere in between.
我们认为这才是完美匹配的算法 结果居于欠拟合与过拟合之间

33
00:01:31.705 --> 00:01:34.260
So in a 2D example like this,
像图上的这个有两个特征值

34
00:01:34.260 --> 00:01:35.610
with just two features,
的2D样例

35
00:01:35.610 --> 00:01:39.700
X-1 and X-2, you can plot the data and visualize bias and variance.
我们把横轴标为x1 纵轴标为x2 你可以绘制数据 把偏差和方差可视化

36
00:01:39.700 --> 00:01:41.250
In high dimensional problems,
在高维度问题中

37
00:01:41.250 --> 00:01:44.735
you can't plot the data and visualize division boundary.
你无法将数据绘制在图上 并在图上进行区分

38
00:01:44.735 --> 00:01:46.830
Instead, there are couple of different metrics,
然而 对应这样的情况也会有几个不同的方法

39
00:01:46.830 --> 00:01:49.750
that we'll look at, to try to understand bias and variance.
让我们来探讨一下 试着去理解偏差和方差的含义

40
00:01:49.750 --> 00:01:51.960
So continuing our example of cat picture classification,
让我们接着讲猫的图片分类的例子

41
00:01:51.960 --> 00:01:57.600
where that's a positive example and that's a negative example,
这里有一个正确的样本和一个错误的样本

42
00:01:57.600 --> 00:02:01.455
the two key numbers to look at and understand bias and variance will be
理解偏差和方差的两个关键数据是

43
00:02:01.455 --> 00:02:06.415
the train set error and the dev set or the development set error.
训练集误差和开发集误差

44
00:02:06.415 --> 00:02:07.716
So for the sake of argument,
为了便于讨论

45
00:02:07.716 --> 00:02:10.290
let's say that you're recognizing cats in pictures,
我们假设你已经在图片中识别出了猫

46
00:02:10.290 --> 00:02:13.860
is something that people can do nearly perfectly, right?
这几乎每个人都能完美做到的 对吧

47
00:02:13.860 --> 00:02:22.155
So let's say, your training set error is 1% and your dev set error is,
所以我们可以理解为 你的训练集误差是1% 而对于开发集误差

48
00:02:22.155 --> 00:02:23.580
for the sake of argument,
为了便于讨论

49
00:02:23.580 --> 00:02:25.585
let's say is 11%.
我们假设是11%

50
00:02:25.585 --> 00:02:26.730
So in this example,
在这个例子中

51
00:02:26.730 --> 00:02:29.495
you're doing very well on the training set,
你的模型对训练集处理得非常好

52
00:02:29.495 --> 00:02:34.355
but you're doing relatively poorly on the development set.
但是相对来说 开发集处理得就有些不尽如人意

53
00:02:34.355 --> 00:02:38.215
So this looks like you might have overfit the training set,
所以这可能是在处理训练集时过拟合了

54
00:02:38.215 --> 00:02:40.620
that somehow you're not generalizing well,
意味着该算法应用到开发集中的所有需要交叉验证的

55
00:02:40.620 --> 00:02:43.815
to this whole cross-validation set in the development set.
数据时 并未处理得十分妥当

56
00:02:43.815 --> 00:02:46.440
And so if you have an example like this,
如果你得到的结果和这个例子一样

57
00:02:46.440 --> 00:02:50.785
we would say this has high variance.
我们将它定义成高方差

58
00:02:50.785 --> 00:02:54.240
So by looking at the training set error and the development set error,
通过观察训练集误差和开发集误差

59
00:02:54.240 --> 00:02:59.730
you would be able to render a diagnosis of your algorithm having high variance.
你将能判断出你的模型算法是否有高方差的问题

60
00:02:59.730 --> 00:03:04.435
Now, let's say, that you measure your training set and your dev set error,
现在让我们再来看第二个例子 假设你得到了训练集和开发集的误差

61
00:03:04.435 --> 00:03:06.095
and you get a different result.
有了与第一个例子不一样的结果

62
00:03:06.095 --> 00:03:09.610
Let's say, that your training set error is 15%.
假设训练集的误差是15%

63
00:03:09.610 --> 00:03:12.375
I'm writing your training set error in the top row,
我把这个训练集误差写在第一行

64
00:03:12.375 --> 00:03:15.880
and your dev set error is 16%.
假设你的开发集误差是16%

65
00:03:15.880 --> 00:03:22.870
In this case, assuming that humans achieve roughly 0% error,
在这种情况下 我们假设人工识别误差是0%

66
00:03:22.870 --> 00:03:27.451
that humans can look at these pictures and just tell if it's cat or not,
因为人可以直接看到这些图片 并判断出这是否是一只猫

67
00:03:27.451 --> 00:03:31.600
then it looks like the algorithm is not even doing very well on the training set.
所以看上去 这个算法在训练集上的表现并不尽如人意

68
00:03:31.600 --> 00:03:35.380
So if it's not even fitting the training data seam that well,
如果它并未将训练集数据处理得很好

69
00:03:35.380 --> 00:03:38.220
then this is underfitting the data.
这就是欠拟合

70
00:03:38.220 --> 00:03:40.895
And so this algorithm has high bias.
我们认为这个算法是高偏差的

71
00:03:40.895 --> 00:03:45.390
But in contrast, this actually generalizing at a reasonable level to the dev set,
但相比之下 这个算法应用在开发集时还处于一个可接受的水准

72
00:03:45.390 --> 00:03:49.365
whereas performance in the dev set is only 1% worse than performance in the training set.
和应用在训练集时的表现相比 误差只多了1%

73
00:03:49.365 --> 00:03:52.355
So this algorithm has a problem of high bias,
所以这个算法的问题是高偏差

74
00:03:52.355 --> 00:03:56.325
because it was not even fitting the training set.
因为它并不能在训练集上对样本进行很好的识别

75
00:03:56.325 --> 00:04:01.030
Well, this is similar to the leftmost plots we had on the previous slide.
这种情况和我们在上一页PPT中最左边的图极为相似

76
00:04:01.030 --> 00:04:03.329
Now, here's another example.
接下来让我们来讲另一个例子

77
00:04:03.329 --> 00:04:06.430
Let's say that you have 15% training set error,
假设你的算法在训练集上的误差是15%

78
00:04:06.430 --> 00:04:08.127
so that's pretty high bias,
这是相当高的偏差

79
00:04:08.127 --> 00:04:11.446
but when you evaluate to the dev set it does even worse,
但当你将该算法应用在开发集时 情况变得更糟

80
00:04:11.446 --> 00:04:13.450
maybe it does 30%.
可能有30%的误差

81
00:04:13.450 --> 00:04:18.175
In this case, I would diagnose this algorithm as having high bias,
在这种情况下 我可以判断出这个算法是高偏差的

82
00:04:18.175 --> 00:04:23.780
because it's not doing that well on the training set, and high variance.
它并没有将训练集处理好 并且还是高方差的

83
00:04:23.780 --> 00:04:27.950
So this has really the worst of both worlds.
这是一种非常 十分 极其糟糕的算法

84
00:04:27.950 --> 00:04:29.325
And one last example,
让我们看看最后一个例子

85
00:04:29.325 --> 00:04:32.605
if you have 0.5 training set error,
假设你有0.5%的训练集误差

86
00:04:32.605 --> 00:04:35.145
and 1% dev set error,
以及1%的开发集误差

87
00:04:35.145 --> 00:04:37.130
then maybe our users are quite happy,
这是让大家都喜闻乐见的算法

88
00:04:37.130 --> 00:04:39.850
that you have a cat classifier with only 1% error,
对猫的分类仅有1%的误差

89
00:04:39.850 --> 00:04:44.340
then just we have low bias and low variance.
所以这个算法是低偏差和低方差的

90
00:04:44.340 --> 00:04:47.610
One subtlety, that I'll just briefly mention that
有一点要注意 我简单地提一下

91
00:04:47.610 --> 00:04:50.955
we'll leave to a later video to discuss in detail,
我们会在之后的视频教程中详细讨论

92
00:04:50.955 --> 00:04:54.188
is that this analysis is predicated on the assumption,
这种分析方法的前提基于真人识别的

93
00:04:54.188 --> 00:04:59.115
that human level performance gets nearly 0% error or,
误差为0%的假设

94
00:04:59.115 --> 00:05:01.749
more generally, that the optimal error,
一般来说 我们称之为理想误差

95
00:05:01.749 --> 00:05:04.225
sometimes called base error,
或者有时我们叫它基本误差

96
00:05:04.225 --> 00:05:10.355
so the base in optimal error is nearly 0%.
基本误差接近0%

97
00:05:10.355 --> 00:05:13.565
I don't want to go into detail on this in this particular video,
在这期视频中我就不展开来说这个问题了

98
00:05:13.565 --> 00:05:18.070
but it turns out that if the optimal error or the base error were much higher, say,
但是如果理想误差或基本误差比较高 假设

99
00:05:18.070 --> 00:05:22.360
it were 15%, then if you look at this classifier,
是15% 那么如果我们继续来看这个分类算法

100
00:05:22.360 --> 00:05:25.460
15% is actually perfectly reasonable for training set and you
15%误差实际上对训练集来说是一个近乎完美的结果了

101
00:05:25.460 --> 00:05:30.120
wouldn't see it as high bias and also a pretty low variance.
你不应该认为它是高偏差的 低方差的算法

102
00:05:30.120 --> 00:05:33.440
So the case of how to analyze bias and variance,
所以如何来分析偏差和方差

103
00:05:33.440 --> 00:05:37.460
when no classifier can do very well, for example,
如果没有分类算法能做好 假设

104
00:05:37.460 --> 00:05:40.833
if you have really blurry images,
如果真的有模糊图像

105
00:05:40.833 --> 00:05:46.081
so that even a human or just no system could possibly do very well,
哪怕是真人或者任何系统都不能进行分类

106
00:05:46.081 --> 00:05:49.237
then maybe base error is much higher,
那基本误差就会非常高

107
00:05:49.237 --> 00:05:52.295
and then there are some details of how this analysis will change.
然后会有一些不同的手段来改变分析方法

108
00:05:52.295 --> 00:05:54.725
But leaving aside this subtlety for now,
但是我们现在先不讨论这个问题

109
00:05:54.725 --> 00:05:57.430
the takeaway is that by looking at
而我们可以发现 通过观察

110
00:05:57.430 --> 00:06:02.676
your training set error you can get a sense of how well you are fitting,
训练集的误差 可以知道你的算法是否至少可以将

111
00:06:02.676 --> 00:06:04.331
at least the training data,
训练集数据处理得很好

112
00:06:04.331 --> 00:06:06.770
and so that tells you if you have a bias problem.
然后总结出是否属于高偏差问题

113
00:06:06.770 --> 00:06:10.190
And then looking at how much higher your error goes,
然后通过观察同一个算法

114
00:06:10.190 --> 00:06:12.965
when you go from the training set to the dev set,
在开发集上的误差提高了多少

115
00:06:12.965 --> 00:06:17.055
that should give you a sense of how bad is the variance problem,
可以知道这个算法是否有高方差问题

116
00:06:17.055 --> 00:06:20.857
so you'll be doing a good job generalizing from a training set to the dev set,
这样你可以很好地把算法从训练集应用至开发集

117
00:06:20.857 --> 00:06:22.645
that gives you sense of your variance.
这会让你意识到方差问题

118
00:06:22.645 --> 00:06:26.210
All this is under the assumption that the base error is quite
上述结果都基于基本误差非常低并且

119
00:06:26.210 --> 00:06:30.235
small and that your training and your dev sets are drawn from the same distribution.
你的训练集和开发集都来自同一个分布的假设

120
00:06:30.235 --> 00:06:32.210
If those assumptions are violated,
如果不满足这些假设

121
00:06:32.210 --> 00:06:34.323
there's a more sophisticated analysis you could do,
那么你需要做一个更复杂的分析

122
00:06:34.323 --> 00:06:36.510
which we'll talk about in the later video.
我们将在之后几期视频教程中讨论

123
00:06:36.510 --> 00:06:38.780
Now, on the previous slide,
好了 在上一张PPT中

124
00:06:38.780 --> 00:06:40.849
you saw what high bias,
我们展示了高偏差

125
00:06:40.849 --> 00:06:42.185
high variance looks like,
和高方差的形态

126
00:06:42.185 --> 00:06:44.920
and I guess you have the sense of what it a good class can look like.
我猜你已经意识到一个优秀的分类器看上去是什么样的了

127
00:06:44.920 --> 00:06:48.110
What does high bias and high variance looks like?
高偏差和高方差的算法看上去如何呢

128
00:06:48.110 --> 00:06:50.535
This is kind of the worst of both worlds.
这是一种无比糟糕的情况

129
00:06:50.535 --> 00:06:53.415
So you remember, we said that a classifier like this,
你应该还记得 我们说过一个像这样的分类器

130
00:06:53.415 --> 00:06:55.755
then your classifier has high bias,
这意味着你的算法是高偏差的

131
00:06:55.755 --> 00:06:58.185
because it underfits the data.
因为欠拟合

132
00:06:58.185 --> 00:07:04.030
So this would be a classifier that is mostly linear and therefore underfits the data,
所以可以说 由于这几乎是个线性分类器(直线) 所以它欠拟合了

133
00:07:04.030 --> 00:07:05.570
we're drawing this is purple.
我们用紫色来标一下

134
00:07:05.570 --> 00:07:09.546
But if somehow your classifier does some weird things,
但是如果在某些情况下你的分类器做了些诡异的事情(像我现在画的这样)

135
00:07:09.546 --> 00:07:14.460
then it is actually overfitting parts of the data as well.
那么实际上这部分属于过拟合了

136
00:07:14.460 --> 00:07:16.500
So the classifier that I drew in purple,
所以我在紫色部分标示的分类器

137
00:07:16.500 --> 00:07:19.455
has both high bias and high variance.
同时具有高偏差和高方差的问题

138
00:07:19.455 --> 00:07:21.300
Where it has high bias, because,
从图上看 这里有高偏差问题

139
00:07:21.300 --> 00:07:23.325
by being a mostly linear classifier,
几乎是一个线性分类器

140
00:07:23.325 --> 00:07:24.875
is just not fitting.
无法拟合分类

141
00:07:24.875 --> 00:07:28.466
You know, this quadratic line shape that well,
你看 就像图上这个曲线形状的类别

142
00:07:28.466 --> 00:07:31.200
but by having too much flexibility in the middle,
但是这个分类器中间 又变得十分扭曲

143
00:07:31.200 --> 00:07:32.995
it somehow gets this example,
它虽然诡异地正确区分了这个叉样本

144
00:07:32.995 --> 00:07:36.720
and this example. Overfits those two examples as well.
和这个圈样本 但这显然就属于过拟合了

145
00:07:36.720 --> 00:07:40.515
So this classifier kind of has high bias because it was mostly linear,
综上所述 我们称这个分类器是高偏差的 因为它几乎是直线

146
00:07:40.515 --> 00:07:43.620
but you need maybe a curve function or quadratic function.
但你需要的可能是一个曲线函数或二次函数

147
00:07:43.620 --> 00:07:45.115
And it has high variance,
同时它也是高方差的

148
00:07:45.115 --> 00:07:49.595
because it had too much flexibility to fit those two mislabel,
因为它在中间用极为扭曲的算法 对两个孤立的

149
00:07:49.595 --> 00:07:52.475
or those live examples in the middle as well.
甚至可能是错误的样本进行了拟合

150
00:07:52.475 --> 00:07:54.300
In case this seems contrived, well,
看上去这比较像人为的

151
00:07:54.300 --> 00:07:57.585
this example is a little bit contrived in two dimensions,
这个样例在二维中看上去不太自然

152
00:07:57.585 --> 00:07:59.883
but with very high dimensional inputs.
但是当你有相当高维度的输入特征

153
00:07:59.883 --> 00:08:01.795
You actually do get things with
你可能在获取特征的过程中

154
00:08:01.795 --> 00:08:04.800
high bias in some regions and high variance in some regions,
在一些地方遇到了高偏差的问题 一些地方遇到了高方差的问题

155
00:08:04.800 --> 00:08:07.410
and so it is possible to get classifiers like this
所以很可能最终得到了在高维度特征输入情况下

156
00:08:07.410 --> 00:08:11.415
in high dimensional inputs that seem less contrived.
产生了这样不自然的分类

157
00:08:11.415 --> 00:08:15.690
So to summarize, you've seen how by looking at your algorithm's error on
让我们总结一下 这节课我们学习了如何通过观察算法

158
00:08:15.690 --> 00:08:20.550
the training set and your algorithm's error on the dev set you can try to diagnose,
在训练集和开发集的误差来诊断

159
00:08:20.550 --> 00:08:23.413
whether it has problems of high bias or high variance,
它是否有高偏差或者高方差的问题

160
00:08:23.413 --> 00:08:25.420
or maybe both, or maybe neither.
或许两者都有 或许都没有

161
00:08:25.420 --> 00:08:28.995
And depending on whether your algorithm suffers from bias or variance,
基于算法遇到高偏差或高方差问题的不同情况

162
00:08:28.995 --> 00:08:31.765
it turns out that there are different things you could try.
我们可以尝试不同的方法来进行改进

163
00:08:31.765 --> 00:08:33.840
So in the next video, I want to present to you,
所以在下一个视频教程中 我希望能给你介绍一种方法

164
00:08:33.840 --> 00:08:37.390
what I call a basic recipe for Machine Learning,
我称之为机器学习的基本准则

165
00:08:37.390 --> 00:08:40.905
that lets you more systematically try to improve your algorithm,
那会让你在遇到高偏差或者高方差问题时

166
00:08:40.905 --> 00:08:44.370
depending on whether it has high bias or high variance issues.
能够更系统地去尝试改进你的算法

167
00:08:44.370 --> 00:08:46.110
So let's go on to the next video.
让我们继续下一期教程
