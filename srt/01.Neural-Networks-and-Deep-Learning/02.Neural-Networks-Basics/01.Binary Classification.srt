1
00:00:00,920 --> 00:00:02,860
Hello, and welcome back.
大家好 欢迎回来


2
00:00:02,860 --> 00:00:08,860
In this week we're going to go over the basics of neural network programming.
这周我们将学习神经网络的基础知识

3
00:00:08,860 --> 00:00:11,990
It turns out that when you implement a neural network there
需要注意的是 当实现一个神经网络的时候

4
00:00:11,990 --> 00:00:16,260
are some techniques that are going to be really important.
我们需要知道一些非常重要的技术和技巧

5
00:00:16,260 --> 00:00:21,150
For example, if you have a training set of m training examples,
例如 有一个包含m个样本的训练集

6
00:00:21,150 --> 00:00:25,110
you might be used to processing the training set by having a for loop
你很可能习惯于用一个for循环

7
00:00:25,110 --> 00:00:28,240
step through your m training examples.
来遍历训练集中的每个样本

8
00:00:28,240 --> 00:00:31,260
But it turns out that when you're implementing a neural network,
但是 当实现一个神经网络的时候

9
00:00:31,260 --> 00:00:34,540
you usually want to process your entire training set
我们通常不直接使用for循环

10
00:00:34,540 --> 00:00:39,040
without using an explicit for loop to loop over your entire training set.
来遍历整个训练集

11
00:00:39,040 --> 00:00:42,940
So, you'll see how to do that in this week's materials.
所以在这周的课程中 你将学会如何处理训练集

12
00:00:42,940 --> 00:00:47,700
Another idea, when you organize
the computation of, in your network,
另外 在神经网络的计算中

13
00:00:47,700 --> 00:00:50,248
usually you have what's called a
通常先有一个叫做前向传导(forward pass)或

14
00:00:50,248 --> 00:00:51,519
forward pass or forward propagation step,
叫做前向传播(foward propagation)的步骤

15
00:00:51,670 --> 00:00:53,647
followed by a backward pass or
接着有一个叫做反向传导(backward pass)

16
00:00:53,647 --> 00:00:56,111
what's called a backward propagation step.
或叫做反向传播(backward propagation)的步骤

17
00:00:56,111 --> 00:01:00,010
And so in this week's materials, you also get an introduction about why
所以这周 我也会向你介绍为什么

18
00:01:00,010 --> 00:01:04,830
the computations, in learning an neural network can be organized in this forward
神经网络的训练过程可以分为前向传播

19
00:01:04,830 --> 00:01:08,010
propagation and a separate backward propagation.
和反向传播两个独立的部分

20
00:01:09,100 --> 00:01:12,620
For this week's materials I want to convey these ideas using
在课程中 我将使用逻辑回归(logistic regression)来传达这些想法

21
00:01:12,620 --> 00:01:16,170
logistic regression in order to make the ideas easier to understand.
以使大家能够更加容易地理解这些概念

22
00:01:16,170 --> 00:01:19,970
But even if you've seen logistic regression before, I think that there'll
即使你之前了解过逻辑回归 我认为

23
00:01:19,970 --> 00:01:23,845
be some new and interesting ideas for you to pick up in this week's materials.
这里还是有些新的、有趣的东西等着你去发现和了解

24
00:01:23,845 --> 00:01:25,815
So with that, let's get started.
所以 现在开始进入正题

25
00:01:25,815 --> 00:01:30,605
Logistic regression is an algorithm for binary classification.
逻辑回归是一个用于二分类(binary classification)的算法

26
00:01:30,605 --> 00:01:33,145
So let's start by setting up the problem.
首先我们从一个问题开始说起

27
00:01:33,145 --> 00:01:36,925
Here's an example of a binary classification problem.
这里有一个二分类问题的例子

28
00:01:36,925 --> 00:01:41,545
You might have an input of an image, like that, and
假如你有一张图片作为输入 比如这只猫

29
00:01:41,545 --> 00:01:47,260
want to output a label to recognize
this image as being either a cat,
然后 如果识别这张图片为猫 则输出标签1作为结果

30
00:01:47,260 --> 00:01:52,140
in which case you output 1,
or not-cat in which case you output 0,
如果识别出不是猫 那么输出标签0作为结果

31
00:01:52,140 --> 00:01:57,740
and we're going to use y to denote the output label.
现在我们可以用字母y来表示输出的结果标签

32
00:01:57,740 --> 00:02:01,550
Let's look at how an image is represented in a computer.
我们来看看一张图片在计算机中是如何表示的

33
00:02:01,550 --> 00:02:05,680
To store an image your computer stores three separate matrices
为了保存一张图片 需要保存三个矩阵

34
00:02:05,680 --> 00:02:09,890
corresponding to the red, green, and blue color channels of this image.
它们分别对应图片中的红、绿、蓝三种颜色通道

35
00:02:10,990 --> 00:02:15,900
So if your input image is 64 pixels by 64 pixels,
如果你的图片大小为64x64像素

36
00:02:15,900 --> 00:02:21,700
then you would have 3 64 by 64 matrices
那么你就有三个规模为64x64的矩阵

37
00:02:21,700 --> 00:02:27,230
corresponding to the red, green and blue pixel intensity values for your images.
分别对应图片中红、绿、蓝三种像素的强度值

38
00:02:27,230 --> 00:02:31,290
Although to make this little slide I drew these as much smaller matrices, so
为了便于表示 这里我画了三个很小的矩阵

39
00:02:31,290 --> 00:02:35,320
these are actually 5 by 4 matrices rather than 64 by 64.
注意它们的规模为5x4 而不是64x64

40
00:02:35,320 --> 00:02:41,640
So to turn these pixel intensity values into a feature vector, what we're
为了把这些像素值放到一个特征向量中

41
00:02:41,640 --> 00:02:48,000
going to do is unroll all of these pixel values into an input feature vector x.
我们需要把这些像素值提取出来 然后放入一个特征向量x

42
00:02:48,000 --> 00:02:53,782
So to unroll all these pixel intensity values into feature vector, what we're
为了把这些像素值转换为特征向量

43
00:02:53,782 --> 00:02:59,580
going to do is define a feature vector x corresponding to this image as follows.
我们需要像下面这样定义一个特征向量x来表示这张图片

44
00:02:59,580 --> 00:03:03,960
We're just going to take all the pixel values 255, 231, and so on.
我们把所有的像素都取出来 例如255、231等等

45
00:03:03,960 --> 00:03:10,827
255, 231, and so on until we've listed all the red pixels.
255、231等等 直到取完所有的红色像素

46
00:03:10,827 --> 00:03:15,737
And then eventually 255 134 255,134 and so
接着最后是255、134、255、134等等

47
00:03:15,737 --> 00:03:20,952
on until we get a long feature vector listing out all the red,
直到得到一个特征向量 把图片中所有的

48
00:03:20,952 --> 00:03:25,570
green and blue pixel intensity values of this image.
红、绿、蓝像素值都列出来

49
00:03:25,570 --> 00:03:31,043
If this image is a 64 by 64 image, the total dimension
如果图片的大小为64x64像素 那么向量x的总维度

50
00:03:31,043 --> 00:03:36,401
of this vector x will be 64 by 64 by 3 because that's
将是64乘以64乘以3 因为这就是

51
00:03:36,401 --> 00:03:41,320
the total numbers we have in all of these matrixes.
三个像素矩阵中像素的总量

52
00:03:41,320 --> 00:03:44,097
Which in this case, turns out to be 12,288,
在这个例子中 结果为12,288

53
00:03:44,097 --> 00:03:47,330
that's what you get if you multiply all those numbers.
如果你把它们乘起来，这就是你将得到的结果

54
00:03:47,330 --> 00:03:51,870
And so we're going to use nx=12288
现在我们用nx=12288

55
00:03:51,870 --> 00:03:55,080
to represent the dimension of the input features x.
来表示输入特征向量x的维度

56
00:03:55,080 --> 00:03:59,280
And sometimes for brevity, I will also just use lowercase n
有时候为了简洁 我会直接用小写的n

57
00:03:59,280 --> 00:04:02,720
to represent the dimension of this input feature vector.
来表示输入特征向量的维度

58
00:04:02,720 --> 00:04:07,510
So in binary classification, our goal is to learn a classifier that can input
所以 在二分类问题中 我们的目标就是习得一个分类器

59
00:04:07,510 --> 00:04:10,760
an image represented by this feature vector x.
它以图片的特征向量x作为输入

60
00:04:10,760 --> 00:04:15,460
And predict whether the corresponding label y is 1 or 0,
然后预测输出结果y为1还是0

61
00:04:15,460 --> 00:04:19,000
that is, whether this is a cat image or a non-cat image.
也就是预测图片中是否有猫

62
00:04:19,000 --> 00:04:21,560
Let's now lay out some of the notation that we'll
接下来我们说明一些在余下课程中

63
00:04:21,560 --> 00:04:23,820
use throughout the rest of this course.
需要用到的一些符号

64
00:04:23,820 --> 00:04:29,453
A single training example is represented by a pair (x, y),
用一对(x, y)来表示一个单独的样本

65
00:04:29,453 --> 00:04:34,446
where x is an x-dimensional feature vector
x代表x维的特征向量

66
00:04:34,446 --> 00:04:39,320
and y, the label, is either 0 or 1.
y表示标签(输出结果) 只能为0或1

67
00:04:39,320 --> 00:04:44,550
Your training sets will comprise lower-case m training examples.
而训练集将由(小写的)m个训练样本组成

68
00:04:44,550 --> 00:04:50,320
And so your training sets will be written (x(1), y(1)) which is the input and output
其中(x(1), y(1))表示第一个样本的输入和输出

69
00:04:50,320 --> 00:04:55,370
for your first training example (x(2), y(2)) for the second training example
(x(2), y(2)) 表示第二个样本的输入和输出

70
00:04:55,370 --> 00:05:01,980
up to (x(m),y(m)) which is your last training example.
直到最后一个样本(x(m),y(m))

71
00:05:01,980 --> 00:05:05,650
And then that altogether is your entire training set.
然后所有的这些一起表示整个训练集

72
00:05:05,650 --> 00:05:10,170
So I'm going to use lowercase m to denote the number of training samples.
我将使用小写的字母m来表示训练样本的个数

73
00:05:10,170 --> 00:05:14,418
And sometimes to emphasize that this is the number of train examples,
有时候为了强调这是训练样本的个数

74
00:05:14,418 --> 00:05:16,437
I might write this as M = M train.
我会写作M=M下标train

75
00:05:16,437 --> 00:05:18,692
And when we talk about a test set,
当涉及到测试集的时候

76
00:05:18,692 --> 00:05:24,430
we might sometimes use m subscript test to denote the number of test examples.
我们会使用m下标test来表示测试集的样本数

77
00:05:24,430 --> 00:05:27,430
So that's the number of test examples.
所以这是测试集的样本数

78
00:05:27,430 --> 00:05:33,440
Finally, to output all of the training examples into a more compact notation,
最后 为了能把训练集表示得更紧凑一点

79
00:05:33,440 --> 00:05:36,840
we're going to define a matrix, capital X.
我们会定义一个矩阵 用大写的X表示

80
00:05:36,840 --> 00:05:41,592
As defined by taking you training set inputs x1, x2 and
它由输入向量x1、x2等等组成

81
00:05:41,592 --> 00:05:44,568
so on and stacking them in columns.
像这样放在矩阵的列中

82
00:05:44,568 --> 00:05:49,958
So we take x1 and put that as a first column of this matrix,
所以现在 我们把x1作为第一列放在矩阵中

83
00:05:49,958 --> 00:05:54,798
x2, put that as a second column and so on down to xm,
x2作为第二列 直到xm放到第m列

84
00:05:54,798 --> 00:05:58,000
then this is the matrix capital X.
然后我们就得到了训练集矩阵(大写的)X

85
00:05:58,000 --> 00:06:03,005
So this matrix X will have m columns, where m is the number of train
所以这个矩阵有m列 m是训练集的样本数量

86
00:06:03,005 --> 00:06:08,665
examples and the number of railroads, or the height of this matrix is NX.
然后这个矩阵的高度记为nX

87
00:06:08,665 --> 00:06:14,400
Notice that in other causes, you might see the matrix capital
注意 有时候可能因为其他某些原因

88
00:06:14,400 --> 00:06:19,390
X defined by stacking up the train examples in rows like so,
矩阵X会由训练样本按照行堆叠起来 而不是列 例如这样

89
00:06:19,390 --> 00:06:23,940
x1 transpose down to xm transpose.
x1的转置直到xm的转置

90
00:06:23,940 --> 00:06:27,704
It turns out that when you're implementing neural networks using
但是在实现神经网络的时候 使用左边的这种形式

91
00:06:27,704 --> 00:06:32,218
this convention I have on the left, will make the implementation much easier.
会让整个实现的过程变得更加简单

92
00:06:32,218 --> 00:06:37,171
So just to recap, X is a nx by m dimensional matrix, and
现在来简单温习一下 X是一个规模为nx乘以m的矩阵

93
00:06:37,171 --> 00:06:40,404
when you implement this in Python,
当你用Python实现的时候

94
00:06:40,404 --> 00:06:45,362
you see that x.shape, that's the python command for
你会看到x.shape 这是一条Python命令

95
00:06:45,362 --> 00:06:50,325
founding the shape of the matrix, that this an nx, m.
用于设置矩阵的规模 即x.shape等于nx逗号m

96
00:06:50,325 --> 00:06:53,255
That just means it is an nx by m dimensional matrix.
表示X是一个规模为nx乘以m的矩阵

97
00:06:53,255 --> 00:06:58,785
So that's how you group the training examples, input x into matrix.
所以综上 这就是如何将训练样本(输入向量x的集合)表示为一个矩阵

98
00:06:58,785 --> 00:07:01,315
How about the output labels y?
那么输出标签y呢？

99
00:07:01,315 --> 00:07:04,815
It turns out that to make your implementation of a neural network easier,
同样的道理 为了能更加容易地实现一个神经网络

100
00:07:04,815 --> 00:07:10,030
it would be convenient to also stack y in columns.
将y标签放在列中将会使得后续计算非常方便

101
00:07:10,030 --> 00:07:14,650
So we're going to define capital Y to be equal to y1, y2
所以我们定义大写的Y等于y1,y2

102
00:07:14,650 --> 00:07:18,580
up to ym like so.
直到ym

103
00:07:18,580 --> 00:07:24,980
So Y here will be a 1 by m dimensional matrix.
所以Y在这里是一个规模为1乘以m的矩阵

104
00:07:24,980 --> 00:07:30,530
And again, to use the Python notation then the shape of Y will be 1, m.
同样地 使用Python将表示为Y.shape等于1逗号m

105
00:07:30,530 --> 00:07:34,810
Which just means this is a 1 by m matrix.
表示这是一个规模为1乘以m的矩阵

106
00:07:34,810 --> 00:07:39,660
And as you implement your neural network, later in this course, you find that a useful
当你在后面的课程中实现神经网络的时候 你会发现

107
00:07:39,660 --> 00:07:43,630
convention would be to take the data associated with different training
一个好的符号约定能够将不同训练样本的数据很好地组织起来

108
00:07:43,630 --> 00:07:48,580
examples, and by data I mean either x or y, or other quantities you see later.
而我所说的数据 不仅包括x或者y 还包括之后你会看到的其他的量

109
00:07:48,580 --> 00:07:49,900
But to take the stuff or
将不同的

110
00:07:49,900 --> 00:07:52,990
the data associated with different training examples and
训练样本的数据提取出来

111
00:07:52,990 --> 00:07:57,430
to stack them in different columns, like we've done here for both x and y.
然后就像刚刚我们对x和y所做的那样 将他们堆叠在矩阵的列中

112
00:07:58,450 --> 00:08:01,380
So, that's the notation we we'll use for logistic regression and for
所以 这就是我们之后会在逻辑回归

113
00:08:01,380 --> 00:08:04,060
neural networks networks later in this course.
和神经网络上要用到的符号的说明

114
00:08:04,060 --> 00:08:07,430
If you ever forget what a piece of notation means, like what is m or
如果有时候你忘了这些符号的意思 比如什么是m

115
00:08:07,430 --> 00:08:08,300
what is n or
或者什么是n

116
00:08:08,300 --> 00:08:12,630
what is something else, we've also posted on the course website a notation guide
或者忘了其他一些东西 我们也会在课程的网站上放上符号说明

117
00:08:12,630 --> 00:08:17,430
that you can use to quickly look up
what any particular piece of notation means.
然后你可以快速地查阅每个具体的符号代表什么意思

118
00:08:17,430 --> 00:08:20,890
So with that, let's go on to the next video where we'll start to fetch out
好了 我们接着到下一个视频 在下个视频中

119
00:08:20,890 --> 00:08:23,190
logistic regression using this notation.
我们将以逻辑回归作为开始

