1
00:00:00,000 --> 00:00:01,530
In the previous video,
在上一个视频中

2
00:00:01,530 --> 00:00:04,227
you saw the logistic regression model.
你看到的是逻辑回归的模型

3
00:00:04,227 --> 00:00:07,526
To train the parameters W and B of the logistic regression model,
为了训练逻辑回归模型的参数w以及b

4
00:00:07,526 --> 00:00:10,570
you need to define a cost function.
你需要定义一个代价函数(cost function)

5
00:00:10,570 --> 00:00:14,430
Let's take a look at the cost function you can use to train logistic regression.
让我们来看一下这个可以用来训练逻辑回归的代价函数

6
00:00:14,430 --> 00:00:18,195
To recap, this is what we had to find from the previous slide.
总的来说 这就是我们从上一张幻灯片中要找的函数

7
00:00:18,195 --> 00:00:22,792
So your output y-hat is sigmoid of w transpose x plus b
所以你的输出y^=sigmoid(wT+b)

8
00:00:22,792 --> 00:00:24,690
where a sigmoid of Z is as defined here.
就像这里定义为sigmoid(z)

9
00:00:24,690 --> 00:00:27,600
So to learn parameters for your model you're given
为了让你的模型来通过学习调整参数

10
00:00:27,600 --> 00:00:31,200
a training set of m training examples and it
你需要给予一个m个样本的训练集

11
00:00:31,200 --> 00:00:34,060
seems natural that you want to find parameters W
这似乎会让你想要在训练集上找到参数w和b

12
00:00:34,060 --> 00:00:37,781
and B so that at least on the training set, the outputs you have.
来得到你的输出

13
00:00:37,781 --> 00:00:40,225
The predictions you have on the training set,
你对训练集的预测值

14
00:00:40,225 --> 00:00:43,260
which we only write as y-hat (i) that that will be close to
我们将它写成y^ 我们希望它会接近于

15
00:00:43,260 --> 00:00:47,720
the ground truth labels y^(i) that you got in the training set.
在训练集中得到的y值

16
00:00:47,720 --> 00:00:52,110
So to throw in a little bit more detail for the equation on top,
所以 为了让上面的方程式更详细一些

17
00:00:52,110 --> 00:00:56,205
we had said that y-hat is as defined at the top for
我们需要说明上面这里定义的y^是对一个训练样本x来说的

18
00:00:56,205 --> 00:01:00,930
a training example x and of course for each training example,
这种形式当然也适用对于每个训练样本

19
00:01:00,930 --> 00:01:03,240
we're using these superscripts with
我们在使用这些带有圆括号的上标

20
00:01:03,240 --> 00:01:07,710
round brackets with parentheses to index and to differentiate examples.
来作索引以及区分样本

21
00:01:07,710 --> 00:01:12,870
Your prediction on training sample (i) which is y-hat (i) is going to
你的训练样本(i)所对应的预测值是y^(i)

22
00:01:12,870 --> 00:01:18,835
be obtained by taking the sigmoid function and applying it to W transpose X,
是用训练样本的wT*x(i)+b然后通过sigmoid函数中得到的

23
00:01:18,835 --> 00:01:25,905
(i) the input that the training example plus B and you can also define Z (i) as follows.
然后你可以把Z定义成这样

24
00:01:25,905 --> 00:01:30,110
Z (i) is equal to the W transpose x (i) plus b.
Z(i)等于wT*x(i)+b

25
00:01:30,110 --> 00:01:31,350
So throughout this course,
所以在这门课里

26
00:01:31,350 --> 00:01:33,966
we're going to use this notational convention,
我们将使用这个符号注解

27
00:01:33,966 --> 00:01:41,605
that the superscript parentheses i refers to data.
就是这个上标(i)来指明数据

28
00:01:41,605 --> 00:01:47,615
X or Y or Z or something else associated with the i-th training example,
表示X或者Y或者Z或者其它数据的第i个训练样本

29
00:01:47,615 --> 00:01:50,885
associated with the i-th example.
所以用第i个样本来表示

30
00:01:50,885 --> 00:01:54,840
That's what the superscript i in parentheses means.
这就是上标(i)的含义

31
00:01:54,840 --> 00:01:57,630
Now, let's see what loss function or error
现在让我们来看看损失函数(loss function)
或叫做误差函数（error function）

32
00:01:57,630 --> 00:02:01,315
function we can use to measure how well our algorithm is doing.
它们可以用来衡量算法的运行情况

33
00:02:01,315 --> 00:02:06,015
One thing you could do is define
the loss when your algorithm outputs
你需要做的第一件事就是定义损失（loss）为

34
00:02:06,015 --> 00:02:12,320
y-hat and the true label as Y to
be maybe the square error or one half a square error.
y^和y的平方差 或者它们平方差的一半

35
00:02:12,320 --> 00:02:14,975
It turns out that you could do this,
结果表明你可以这样做

36
00:02:14,975 --> 00:02:17,670
but in logistic regression people don't usually do
但是通常在逻辑回归中大家都不这么做

37
00:02:17,670 --> 00:02:21,000
this because when you come to learn the parameters,
这是因为当你学习这些参数的时候

38
00:02:21,000 --> 00:02:25,682
you find that the optimization problem which we talk about later becomes non-convex.
你会发现我们后来讨论的优化目标变成非凸的

39
00:02:25,682 --> 00:02:30,105
So you end up with optimization problem with multiple local optima.
你最后会得到很多个局部最优值

40
00:02:30,105 --> 00:02:33,285
So gradient descent may not find the global optimum.
所以梯度下降可能不会找到全局最优值

41
00:02:33,285 --> 00:02:35,580
If you didn't understand the last couple of comments.
如果你不知道最后这几句话

42
00:02:35,580 --> 00:02:38,320
Don't worry about it, we'll get to it in later video.
别担心 我们会在后面的视频中讲到它

43
00:02:38,320 --> 00:02:40,990
But the intuition to take away is that
但是这个的直观理解就是

44
00:02:40,990 --> 00:02:44,620
this function L called the loss function is a function you'll
我们通过这个称为L的损失函数

45
00:02:44,620 --> 00:02:51,265
need to define to measure how good our
output y-hat is when the true label is y.
来衡量你的预测输出值y^和y的实际值有多接近

46
00:02:51,265 --> 00:02:54,345
As square error seems like it might be a reasonable choice
平方误差看起来似乎是一个合理的选择

47
00:02:54,345 --> 00:02:58,160
except that it makes gradient descent not work well.
但是在这里它不能使梯度下降很好地工作

48
00:02:58,160 --> 00:03:00,500
So in logistic regression, we will actually define
所以在逻辑回归中我们实际上将会定义

49
00:03:00,500 --> 00:03:05,695
a different loss function that
plays a similar role as squared error,
一个不同的损失函数 它起着与平方误差相似的作用

50
00:03:05,695 --> 00:03:08,910
that will give us an optimization problem that is
这些会给我们一个凸的优化目标

51
00:03:08,910 --> 00:03:13,530
convex and so we'll see in that later video becomes much easier to optimize.
因此我们将会在后面的视频中看到它变得很容易优化

52
00:03:13,530 --> 00:03:17,310
So, what we use in logistic regression is actually
我们在逻辑回归中所用的实际就是

53
00:03:17,310 --> 00:03:21,795
the following loss function which I'm just like right up here,
我这里指的损失函数

54
00:03:21,795 --> 00:03:31,740
is negative y log y-hat plus one minus y log,
它是一个-(y*log(y^) + (1-y)log(1-y^))

55
00:03:31,740 --> 00:03:34,600
one minus y-hat.
它是一个-(y*log(y^) + (1-y)log(1-y^))

56
00:03:34,600 --> 00:03:38,785
Here's some intuition for why this loss function makes sense.
这里有一些直观理解 为什么这个损失函数能够起作用

57
00:03:38,785 --> 00:03:41,285
Keep in mind that if we're using
记得如果我们使用

58
00:03:41,285 --> 00:03:45,820
squared error then you want the squared error to be as small as possible.
平方误差时你会想要让这个误差尽可能地小

59
00:03:45,820 --> 00:03:48,680
And with this logistic regression loss function,
那么对于这个逻辑回归损失函数

60
00:03:48,680 --> 00:03:51,495
we'll also want this to be as small as possible.
我们同样也想让它尽可能地小

61
00:03:51,495 --> 00:03:53,508
To understand why this makes sense,
为了更好地理解为什么它能够起作用

62
00:03:53,508 --> 00:03:55,260
let's look at the two cases.
让我们来看两个例子

63
00:03:55,260 --> 00:03:56,570
In the first case,
在第一个例子中

64
00:03:56,570 --> 00:03:59,430
let's say Y is equal to one then the loss
我们说y等于1时

65
00:03:59,430 --> 00:04:05,415
function y-hat comma y is justice for us to write this negative sign.
损失函数(y^,y)就相当于这里的-ylog(y^)

66
00:04:05,415 --> 00:04:08,735
So this negative log y-hat.
就是-log(y^)

67
00:04:08,735 --> 00:04:10,770
If y is equal to one. Because if y equals
如果y等于1

68
00:04:10,770 --> 00:04:14,070
one then the second term one minus Y is equal to zero.
那么这第二项这里的1-y就等于0

69
00:04:14,070 --> 00:04:19,880
So this says if y equals one you want
negative log y-hat to be as big（little） as possible.
这就是说当y等于1时 你想要把-log(y^)变得尽可能的小（口误）

70
00:04:19,880 --> 00:04:26,040
So that means you want log y-hat to be large,
这意味着你想要把log(y^)变得尽可能的大

71
00:04:26,040 --> 00:04:32,935
to be as big as possible and that means you want y-hat to be large.
这样就意味着你想要y^变得很大

72
00:04:32,935 --> 00:04:35,170
But because y-hat is you know,
但是因为y^是上面所写的

73
00:04:35,170 --> 00:04:38,440
the sigmoid function, it can never be bigger than one.
sigmoid函数 它永远不会比1大

74
00:04:38,440 --> 00:04:41,850
So this is saying that if y is equal to one you,
所以这也就是说如果y等于1时

75
00:04:41,850 --> 00:04:44,050
want y-hat to be as big as possible.
你会想让y^变得尽可能地大

76
00:04:44,050 --> 00:04:48,220
But it can't ever be bigger than one so saying you want y-hat to be close to one as well.
但它永远不会大于1 所以说你要让y^接近1

77
00:04:48,220 --> 00:04:50,740
The other case is if y equals zero.
另一个一个例子就是如果y等于0

78
00:04:50,740 --> 00:04:55,375
If y equals zero then this first term
in the loss function is equal to zero because
如果y等于0 那么损失函数的第一项等于0

79
00:04:55,375 --> 00:05:01,290
y zero and then the second term defines the loss function.
因为y是0 然后第二项就是这个损失函数

80
00:05:01,290 --> 00:05:07,210
So the loss becomes negative log one minus y-hat.
所以这损失函数变成-log(1-y^)

81
00:05:07,210 --> 00:05:11,480
And so if in your learning procedure you try to make the loss function small,
如果在学习过程中你试图让损失函数小一些

82
00:05:11,480 --> 00:05:19,450
what this means is that you want log one minus y-hat to be large.
也就意味着你想要log(1-y^)变得很大

83
00:05:19,450 --> 00:05:22,050
And because it's a negative sign there and
因为这里是一个负号

84
00:05:22,050 --> 00:05:24,660
then through a similar piece of reason you can conclude
然后通过这一系列类似的理论你可以得出结论

85
00:05:24,660 --> 00:05:30,870
that this loss function is trying to make y-hat as small as possible.
就是这个损失函数正在让y^尽可能的小

86
00:05:30,870 --> 00:05:34,320
And again because y-hat has to be between zero and one.
再者 因为y^只能位于0到1之间

87
00:05:34,320 --> 00:05:38,155
This is saying that if y is equal to zero then
这就是说y等于0时

88
00:05:38,155 --> 00:05:43,790
your loss function will push the parameters to make y-hat as close to zero as possible.
你的损失函数会让这些参数让y^尽可能地接近于0

89
00:05:43,790 --> 00:05:48,305
Now, there are a lot of functions with roughly this effect that if y is equal
在这门课里会有许多的函数的效果和现在这个类似 就是如果y等于1

90
00:05:48,305 --> 00:05:52,950
to one we try to make y-hat large and if Y is equal to zero we try to make y-hat small.
我们就尽可能让y^变大 如果y等于0 我们尽可能让y^变小

91
00:05:52,950 --> 00:05:55,150
We just gave here in green
我们这里用绿色来写

92
00:05:55,150 --> 00:05:59,920
a somewhat informal justification for this loss function? we'll provide
关于为什么用这个损失函数 我们稍后将会提供一个

93
00:05:59,920 --> 00:06:03,970
an optional video later to give a more formal justification for
可选择性观看的视频 来给出一个更正式的理由

94
00:06:03,970 --> 00:06:08,500
why in logistic regression we like to use the loss function with this particular form.
解释为什么在逻辑回归中我们喜欢使用这种特殊形式的损失函数

95
00:06:08,500 --> 00:06:13,630
{\an8}Finally, the loss function was defined with respect to a single training example.
最后 损失函数是在单个训练样本中定义的

96
00:06:13,630 --> 00:06:16,760
{\an8}It measures how well you're doing on a single training example.
它衡量的是你在单个训练样本上的表现如何

97
00:06:16,760 --> 00:06:21,148
{\an8}I'm now going to define something called the cost function,
现在我要定义一个代价函数(cost function)

98
00:06:21,148 --> 00:06:24,690
{\an8}which measures how well you're doing an entire training set.
它衡量的是你在全部训练样本上的表现如何

99
00:06:24,690 --> 00:06:28,660
{\an8}So the cost function J which is applied to
所以这个代价函数J

100
00:06:28,660 --> 00:06:33,130
{\an8}your parameters W and B is going to be the average with one of
根据之前得到的两个参数w和b J(w, b)等于

101
00:06:33,130 --> 00:06:43,270
{\an8}the m of the sum of the loss function applied to each of the training examples and turn.
1/m乘以L(y^(i), y(i))的求和

102
00:06:43,270 --> 00:06:45,435
{\an8}While here y-hat is of course
而y^是

103
00:06:45,435 --> 00:06:49,570
{\an8}the prediction output by your logistic regression algorithm using you know,
通过逻辑回归算法 用一组特定的参数w和b

104
00:06:49,570 --> 00:06:52,430
{\an8}a particular set of parameters W and B.
得出的预测输出值

105
00:06:52,430 --> 00:06:54,480
{\an8}And so just to expand this out,
所以把这个展开

106
00:06:54,480 --> 00:06:58,010
{\an8}this is equal to negative one over
这等于-1/m

107
00:06:58,010 --> 00:07:03,550
{\an8}m sum from i equals one through m of the definition of the loss function.
从i=1到m对损失函数的求和

108
00:07:03,550 --> 00:07:07,530
{\an8}So this is y (i) Log y-hat
这是y(i)*log(y^(i))

109
00:07:07,530 --> 00:07:14,530
{\an8}(i) plus minus is y (i) log one minus y-hat (i).
+ (1 - y(i))*log(1 - y^(i))

110
00:07:14,530 --> 00:07:17,880
{\an8}I guess I could put square brackets here.
我想我可以在这里画上方括号

111
00:07:17,880 --> 00:07:20,945
{\an8}So the minus sign is outside everything else.
负号在外面

112
00:07:20,945 --> 00:07:23,665
So the terminology I'm going to use is that
这里我要用的术语是

113
00:07:23,665 --> 00:07:29,120
the loss function is applied to just a single training example like so.
损失函数只适用于像这样的单个训练样本

114
00:07:29,120 --> 00:07:33,010
And the cost function is the cost of your parameters.
这个代价函数是你参数的总代价

115
00:07:33,010 --> 00:07:36,115
So in training your logistic regression model,
所以在训练你的逻辑回归模型时

116
00:07:36,115 --> 00:07:38,980
we're going to try to find parameters W and B that
我们试图找到合适的参数w和b

117
00:07:38,980 --> 00:07:43,475
minimize the overall cost function J written at the bottom.
来将屏幕下方写的代价函数J的总代价降到最低

118
00:07:43,475 --> 00:07:48,040
So, you've just seen the set up for the logistic regression algorithm,
所以你们刚刚看到了逻辑回归算法的推导

119
00:07:48,040 --> 00:07:50,770
the loss function for training example and the
以及针对单个训练样本的损失函数

120
00:07:50,770 --> 00:07:54,190
overall cost function for the parameters of your algorithm.
和针对你算法所选用参数的总代价函数

121
00:07:54,190 --> 00:07:59,485
It turns out that logistic regression can be viewed as a very very small neural network.
结果表明逻辑回归可以被看作是一个非常小的神经网络

122
00:07:59,485 --> 00:08:01,905
In the next video we'll go over that so you can start
在下一个视频里 我们将会提到

123
00:08:01,905 --> 00:08:04,965
gaining intuition about what neural networks do.
关于神经网络能做什么的直观理解

124
00:08:04,965 --> 00:08:08,230
So that let's go onto the next video about how to
下面我们来看下一段视频

125
00:08:08,230 --> 00:08:11,630
view logistic regression as a very small neural network.
关于如何将逻辑回归看作一个非常小的神经网络

